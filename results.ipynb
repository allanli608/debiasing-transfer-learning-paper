{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30d49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026b82da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "# Enable hot-reloading so if you edit src/train.py, it updates here immediately\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from src import MBartNeutralizer, WNCDataset, WeightedSeq2SeqTrainer\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6b40f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing mBART on cuda...\n"
     ]
    }
   ],
   "source": [
    "neutralizer = MBartNeutralizer(model_name=\"facebook/mbart-large-50\")\n",
    "model = neutralizer.get_model()\n",
    "tokenizer = neutralizer.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d07f87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([128])\n",
      "Labels Shape: torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the filtered \"Complex\" dataset created by preprocess.py\n",
    "train_set = WNCDataset(\"data/processed/train_complex.csv\", tokenizer)\n",
    "val_set = WNCDataset(\"data/processed/val_complex.csv\", tokenizer)\n",
    "\n",
    "# Sanity Check: Print one example\n",
    "sample = train_set[0]\n",
    "print(\"Input Shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Labels Shape:\", sample[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057c24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12923/1370919502.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedSeq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU VRAM\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,  # Lower LR for fine-tuning\n",
    "    logging_steps=100,\n",
    "    save_steps=1500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1500,\n",
    "    fp16=True,  # Essential for mBART memory efficiency\n",
    "    remove_unused_columns=False,  # IMPORTANT: Keep 'loss_weights' in the batch\n",
    ")\n",
    "\n",
    "trainer = WeightedSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a0da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16968' max='16968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16968/16968 2:18:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.353512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.339190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.328251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.328729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.327250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.322399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.322384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.331449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.330756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>0.330291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.208100</td>\n",
       "      <td>0.328351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This will output the live loss curve\n",
    "# train_result = trainer.train()\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351b5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/mbart_neutralizer_en_v1\n",
      "Output: the regime failed to act.\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned weights\n",
    "neutralizer.save_model(\"models/mbart_neutralizer_en_v1\")\n",
    "\n",
    "# Quick Inference Test\n",
    "input_text = \"The radical regime failed to act.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae7f19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model for final verification...\n",
      "Initializing mBART on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models/mbart_neutralizer_en_v1' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PHASE 1 COMPLETE: ENGLISH BASELINE ====================\n",
      "\n",
      "Original: The radical regime failed to act on the crisis.\n",
      "Neutral:  the iranian government failed to act on the crisis.\n",
      "--------------------------------------------------\n",
      "Original: The controversial politician foolishly denied the allegations.\n",
      "Neutral:  the controversial politician denied the allegations.\n",
      "--------------------------------------------------\n",
      "Original: He exposed the senator's corruption.\n",
      "Neutral:  he accused the senator's corruption.\n",
      "--------------------------------------------------\n",
      "\n",
      "If the 'Neutral' outputs removed the biased words (radical, foolishly, exposed)\n",
      "while keeping the facts, Phase 1 is SUCCESSFUL.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading fine-tuned model for final verification...\")\n",
    "# Make sure this matches the path you saved to\n",
    "saved_path = \"models/mbart_neutralizer_en_v1\" \n",
    "neutralizer = MBartNeutralizer(model_name=saved_path)\n",
    "model = neutralizer.get_model()\n",
    "tokenizer = neutralizer.get_tokenizer()\n",
    "\n",
    "# 2. Define \"The Gauntlet\" (Test Cases)\n",
    "test_cases = [\n",
    "    # Case 1: Subjective Intensifier (Easy)\n",
    "    \"The radical regime failed to act on the crisis.\",\n",
    "    \n",
    "    # Case 2: Framing Bias (Harder - subtle verb change)\n",
    "    \"The controversial politician foolishly denied the allegations.\",\n",
    "    \n",
    "    # Case 3: Presupposition (Hardest - implies guilt)\n",
    "    \"He exposed the senator's corruption.\" \n",
    "]\n",
    "\n",
    "# 3. Run Robust Inference\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\n{'='*20} PHASE 1 COMPLETE: ENGLISH BASELINE {'='*20}\\n\")\n",
    "\n",
    "for text in test_cases:\n",
    "    # A. Tokenize (Force English Source)\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # B. Generate (Prevent Repetition & Force English Output)\n",
    "    generated_ids = model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"], \n",
    "        max_length=64,\n",
    "        num_beams=5,             # Smarter search\n",
    "        no_repeat_ngram_size=2,  # Prevents \"same same\" loops\n",
    "        repetition_penalty=1.2,  # Soft penalty to encourage natural phrasing\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # C. Decode\n",
    "    output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # D. Display\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Neutral:  {output}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nIf the 'Neutral' outputs removed the biased words (radical, foolishly, exposed)\")\n",
    "print(\"while keeping the facts, Phase 1 is SUCCESSFUL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47864e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- INITIALIZING PHASE 3: SYNTHETIC CHINESE TRAINING ---\")\n",
    "\n",
    "# 1. Initialize mBART for Chinese-to-Chinese (zh_CN)\n",
    "# We use a fresh model instance (not the English-finetuned one) for Model 3.\n",
    "# If you wanted to do \"Transfer Learning\" (Model 2 -> 3), you would load \"models/mbart_neutralizer_en_v1\" instead.\n",
    "# For now, let's train from scratch (Base mBART) on the synthetic data to compare fairly.\n",
    "neutralizer = MBartNeutralizer(\n",
    "    model_name=\"facebook/mbart-large-50\", \n",
    "    src_lang=\"zh_CN\", \n",
    "    tgt_lang=\"zh_CN\"\n",
    ")\n",
    "model = neutralizer.get_model()\n",
    "tokenizer = neutralizer.get_tokenizer()\n",
    "\n",
    "# 2. Load the Synthetic Chinese Data\n",
    "# Ensure your translation script has finished and these files exist!\n",
    "train_path = \"data/processed/train_chinese_synthetic.csv\"\n",
    "val_path = \"data/processed/val_chinese_synthetic.csv\"\n",
    "\n",
    "print(f\"Loading datasets from {train_path}...\")\n",
    "train_set = WNCDataset(train_path, tokenizer)\n",
    "val_set = WNCDataset(val_path, tokenizer)\n",
    "\n",
    "# Sanity Check: Verify the first sample looks like Chinese\n",
    "print(f\"Sample Input ID 0: {train_set[0]['input_ids'][0]}\") \n",
    "print(\"Language Code ID for zh_CN:\", tokenizer.lang_code_to_id[\"zh_CN\"]) \n",
    "# The first token of input_ids SHOULD match the zh_CN ID.\n",
    "\n",
    "# 3. Configure Training (Safe Settings for RTX 4090)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_zh\",          # <--- NEW OUTPUT DIR\n",
    "    per_device_train_batch_size=4,      # 4090 can handle 4 easily with fp16\n",
    "    gradient_accumulation_steps=8,      # Effective Batch Size = 32\n",
    "    gradient_checkpointing=True,        # Save VRAM\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,                 # Standard fine-tuning rate\n",
    "    logging_steps=100,\n",
    "    save_steps=1500,                     # Save checkpoint every 500 steps\n",
    "    save_total_limit=2,                 # Keep disk clean\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1500,                    # Evaluate less often to save time\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# 4. Initialize Trainer\n",
    "trainer = WeightedSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train\n",
    "trainer.train()\n",
    "\n",
    "# 6. Save Final Model\n",
    "output_path = \"models/mbart_neutralizer_zh_synthetic\"\n",
    "neutralizer.save_model(output_path)\n",
    "print(f\"Phase 3 Complete. Model saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
