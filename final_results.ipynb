{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20496bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 test cases.\n",
      "\n",
      "ğŸš€ Running Inference for: Baseline (Pivot)...\n",
      "[PivotBaseline] Initializing on cuda...\n",
      "Loading ZH->EN Translator...\n",
      "Loading EN->ZH Translator...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9736c43149a4c21bc8932e541176376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  13%|#3        | 41.9M/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English Neutralizer...\n",
      "Initializing mBART on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models/mbart_neutralizer_en_v1' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Running Inference for: Model 2 (ZeroShot)...\n",
      "[ZeroShotNeutralizer] Initializing on cuda...\n",
      "Initializing mBART on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models/mbart_neutralizer_en_v1' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Running Inference for: Model 3 (Synthetic)...\n",
      "[SyntheticNeutralizer] Initializing on cuda...\n",
      "Initializing mBART on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models/mbart_neutralizer_zh_synthetic' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Inference Complete. Starting Evaluation...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "from src.final_models import PivotBaseline, ZeroShotNeutralizer, SyntheticNeutralizer\n",
    "from src.evaluate import Evaluator\n",
    "\n",
    "# 1. Load the Gold Test Data\n",
    "# Ensure you ran 'src/data_collection/create_pseudo_gold.py' first!\n",
    "test_df = pd.read_csv(\"data/processed/test_chinese_gold.csv\")\n",
    "inputs = test_df[\"Chinese_Biased\"].tolist()\n",
    "gold_refs = test_df[\"Chinese_Neutral_Gold\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(inputs)} test cases.\")\n",
    "\n",
    "# Dictionary to store outputs from each model\n",
    "model_outputs = {}\n",
    "\n",
    "# Define the models we want to test\n",
    "# We use a list of classes so we can instantiate -> run -> delete to save RAM\n",
    "model_classes = [\n",
    "    (\"Baseline (Pivot)\", PivotBaseline),\n",
    "    (\"Model 2 (ZeroShot)\", ZeroShotNeutralizer),\n",
    "    (\"Model 3 (Synthetic)\", SyntheticNeutralizer),\n",
    "]\n",
    "\n",
    "for name, ModelClass in model_classes:\n",
    "    print(f\"\\nğŸš€ Running Inference for: {name}...\")\n",
    "\n",
    "    try:\n",
    "        # A. Initialize Model\n",
    "        model_instance = ModelClass()\n",
    "\n",
    "        # B. Generate\n",
    "        # Using batch_debias if available, else loop\n",
    "        outputs = [model_instance.debias(text) for text in inputs]\n",
    "        model_outputs[name] = outputs\n",
    "\n",
    "        # C. Cleanup (CRITICAL for GPU Memory)\n",
    "        del model_instance\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to run {name}: {e}\")\n",
    "        model_outputs[name] = [\"Error\"] * len(inputs)\n",
    "\n",
    "print(\"\\nâœ… Inference Complete. Starting Evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ’¾ Exporting model outputs for manual inspection...\")\n",
    "\n",
    "# 1. Create a dictionary with the static columns\n",
    "inspection_data = {\n",
    "    \"Original_Biased\": inputs,\n",
    "    \"Gold_Reference\": gold_refs\n",
    "}\n",
    "\n",
    "# 2. Add each model's predictions dynamically\n",
    "for model_name, predictions in model_outputs.items():\n",
    "    # Handle cases where a model might have failed\n",
    "    if len(predictions) != len(inputs):\n",
    "        print(f\"Warning: {model_name} output length mismatch. Padding with blanks.\")\n",
    "        predictions = predictions + [\"\"] * (len(inputs) - len(predictions))\n",
    "        \n",
    "    inspection_data[f\"Pred_{model_name}\"] = predictions\n",
    "\n",
    "# 3. Create DataFrame and Save\n",
    "df_inspection = pd.DataFrame(inspection_data)\n",
    "output_path = \"report_data/manual_inspection.csv\"\n",
    "df_inspection.to_csv(output_path, index=False, encoding='utf-8-sig') # utf-8-sig for proper Excel Chinese rendering\n",
    "\n",
    "print(f\"âœ… Saved manual inspection file to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Evaluators on cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3811454f504ddfb396edcb8346d3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd43d7bc55540e39b721dba0bc11f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f5d447b54f4f5f98a422565ac65ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ac452e7e804e37b348280061a8f993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5790622a7b4af788ec1211457cf9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffe76d8805940df831f51358463077d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc136468d709f17a04c/200c7be40263b94c25592e660b12ffc05e4b242b85310328f6425a60db424882?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251218%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251218T065559Z&X-Amz-Expires=3600&X-Amz-Signature=5bd463f3ee5dabaf4f2f720dd4a7fb551285a496db51ede179f9e5e839df97e0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=62171e3b6a99db28e0b3159d&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&x-id=GetObject&Expires=1766044559&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NjA0NDU1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxN2EwNGMvMjAwYzdiZTQwMjYzYjk0YzI1NTkyZTY2MGIxMmZmYzA1ZTRiMjQyYjg1MzEwMzI4ZjY0MjVhNjBkYjQyNDg4MioifV19&Signature=fc3dySkjaPDKKwg%7EYKb178cYiRsR16%7E0TIPaTor6uIM23lgggkbi2c3x3Nfg4ihQdcA2lVmghLPQOZetL9hPTmIl793V8dgQwfWx28CrTB540z2BLOOV7-VGyrcPBC-EPDEoyoRTYxgKJ9Tj8aSLgq4zEsQKUsHsunujkG-N1pi85hYPnBPDOtLqYPk36RwqOH8EPtSNrhPa9FJtvhOwtwR4NM1X6bHE8Jqtp2CYp%7EjMs3uuTY3dfufNoP97BoQkMwf80ChryiHFCNKggiUjdJVJltgZ-cP0fWGQLIMgiIN4XHoeyPVqLPUqZ2AqdymstU7LFT6pWOZ9f1Bng7azGg__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cba73f880ce439290058853abdaacca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluating Baseline (Pivot)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a4114e48b2449db6fc393ab6dbad24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f173474f66a94f62b3b832e2f554e1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluating Model 2 (ZeroShot)...\n",
      "\n",
      "ğŸ“Š Evaluating Model 3 (Synthetic)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc136468d709f17a04c/0b4df0cd3a2f6077307c86a8b3870ccfe613f6dcc3ec05edf840bbc2b6f2754d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251218%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251218T065753Z&X-Amz-Expires=3600&X-Amz-Signature=3560150841320089231923efd4e6b814454ebf89e4d1ab79bd15bef2c6d1734c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=62171e3b6a99db28e0b3159d&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1766044673&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NjA0NDY3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxN2EwNGMvMGI0ZGYwY2QzYTJmNjA3NzMwN2M4NmE4YjM4NzBjY2ZlNjEzZjZkY2MzZWMwNWVkZjg0MGJiYzJiNmYyNzU0ZCoifV19&Signature=dUh5lGuirrIfIxVwt5aDBcPJgSetJiC-xtV%7EyLDvg9m6WNeFcxaDxgpIIKU8UHkgPNEwT9uMljDuqIuGQ5cP7pFqtlCuwlJ1tvEMGIzXvIxnrTFZgVOmGCYi%7Eg46-EEdzHFSJ0%7E99TZYKusu00Tgd0lKXwCbxz4iAohZQaZbY3yVCcAtvKAM023E1UI1UURT0Qr2H7xlIuxk0DRJsdR1lyZanOVRShQol5zAzF6EfnEm1CafbCJvBvLdkJ4OhOGjhXhJsE772vBkz%7EuoQngrcz4cr1gCBqwpCx8JlQB4hC%7EwMoeJc%7EEONWbczp6k7vfrUEPE75TL5QLzWfwqKYx4HA__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Evaluator (Loads BERT Judge + GPT2)\n",
    "evaluator = Evaluator()\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for model_name, predictions in model_outputs.items():\n",
    "    print(f\"\\nğŸ“Š Evaluating {model_name}...\")\n",
    "\n",
    "    # Check for empty/error outputs\n",
    "    if not predictions or predictions[0] == \"Error\":\n",
    "        continue\n",
    "\n",
    "    # A. Style Accuracy (Did it remove bias?)\n",
    "    acc = evaluator.get_style_accuracy(predictions)\n",
    "\n",
    "    # B. Content Preservation (Did it keep meaning?)\n",
    "    # Compare Prediction vs Original Biased Input (or Gold Neutral if you prefer)\n",
    "    # Comparing to Input checks if we kept the *topic*.\n",
    "    # Comparing to Gold checks if we matched the *reference*.\n",
    "    # Standard Style Transfer usually compares to INPUT for preservation.\n",
    "    bert_score = evaluator.get_bert_score(predictions, inputs)\n",
    "\n",
    "    # C. Fluency (Is it natural Chinese?)\n",
    "    ppl = evaluator.get_perplexity(predictions)\n",
    "\n",
    "    # D. Composite Score (Geometric Mean)\n",
    "    # We use 100/PPL scaling just to make the number readable in the table (e.g., 0.45 instead of 0.004)\n",
    "    # Formula: (Acc * BERT * (100/PPL))^(1/3)\n",
    "    composite = evaluator.calculate_composite_score(acc, bert_score, ppl)\n",
    "\n",
    "    results_table.append(\n",
    "        {\n",
    "            \"Model\": model_name,\n",
    "            \"Style Acc (â†‘)\": f\"{acc:.2%}\",\n",
    "            \"Content Sim (â†‘)\": f\"{bert_score:.3f}\",\n",
    "            \"Fluency PPL (â†“)\": f\"{ppl:.2f}\",\n",
    "            \"Composite Score\": f\"{composite:.3f}\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b25a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FINAL RESEARCH RESULTS \n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Style Acc (â†‘)</th>\n",
       "      <th>Content Sim (â†‘)</th>\n",
       "      <th>Fluency PPL (â†“)</th>\n",
       "      <th>Composite Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline (Pivot)</td>\n",
       "      <td>72.00%</td>\n",
       "      <td>0.835</td>\n",
       "      <td>36688.70</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2 (ZeroShot)</td>\n",
       "      <td>53.00%</td>\n",
       "      <td>0.950</td>\n",
       "      <td>23755.12</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3 (Synthetic)</td>\n",
       "      <td>62.00%</td>\n",
       "      <td>0.964</td>\n",
       "      <td>25694.13</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Style Acc (â†‘) Content Sim (â†‘) Fluency PPL (â†“)  \\\n",
       "0     Baseline (Pivot)        72.00%           0.835        36688.70   \n",
       "1   Model 2 (ZeroShot)        53.00%           0.950        23755.12   \n",
       "2  Model 3 (Synthetic)        62.00%           0.964        25694.13   \n",
       "\n",
       "  Composite Score  \n",
       "0           0.025  \n",
       "1           0.028  \n",
       "2           0.029  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Qualitative Analysis (First 3 Examples) ---\n",
      "\n",
      "Input: å…¶è½´å¿ƒæ˜¯1940å¹´ç­¾ç½²ä¸‰æ–¹æ¡çº¦åå»ºç«‹çš„,å¥‰è¡Œå¼ºå¤§çš„å†›å›½ä¸»ä¹‰ã€ç§æ—ä¸»ä¹‰å’Œæ°‘æ—ä¸»ä¹‰æ„è¯†å½¢æ€,å¥‰è¡Œåå…±æ”¿ç­–ã€‚\n",
      "[Baseline (Pivot)]: å…¶è½´å¿ƒæ˜¯åœ¨ç­¾ç½²ä¸‰æ–¹æ¡çº¦å’Œåå…±æ”¿ç­–ä¹‹åäº1940å¹´ç¡®ç«‹çš„ã€‚\n",
      "[Model 2 (ZeroShot)]: å…¶è½´å¿ƒæ˜¯1940å¹´ç­¾ç½²ä¸‰æ–¹æ¡çº¦åå»ºç«‹çš„,å¥‰è¡Œå¼ºå¤§çš„å†›å›½ä¸»ä¹‰ã€ç§æ—ä¸»ä¹‰å’Œæ°‘æ—ä¸»ä¹‰æ„è¯†å½¢æ€,å¹¶å¥‰è¡Œçš„åå…±æ”¿ç­–ã€‚\n",
      "[Model 3 (Synthetic)]: å…¶è½´å¿ƒæ˜¯1940å¹´ç­¾ç½²ä¸‰æ–¹æ¡çº¦åå»ºç«‹çš„,å¥‰è¡Œåå…±æ”¿ç­–ã€‚\n",
      "\n",
      "Input: ä½åœ¨Jerusalimçš„Pisgat ze'evç¤¾åŒºã€‚\n",
      "[Baseline (Pivot)]: ä»–ä½åœ¨è€¶é²æ²™æ—çš„çš®æ–¯åŠ ç‰¹ç¤¾åŒº\n",
      "[Model 2 (ZeroShot)]: ä½åœ¨Jerusalem,ä½åœ¨Pisgat ze'evç¤¾åŒºã€‚\n",
      "[Model 3 (Synthetic)]: ä½åœ¨Jerusalimçš„Pisgat ze'evç¤¾åŒºã€‚\n",
      "\n",
      "Input: åŒ…æ‹¬ç‘å®‰(ç•¥å¾®æ˜¯ä»–ä»¬çš„é«˜å¹´çº§)çš„çƒå‘˜åœ¨1992å¹´5æœˆå¸®åŠ©ä¿±ä¹éƒ¨èµ¢å¾—äº†é’æ˜¥æ¯,\n",
      "[Baseline (Pivot)]: åŒ…æ‹¬ç‘å®‰(ç•¥å¾®é«˜é¾„)åœ¨å†…, ååŠ©ä¿±ä¹éƒ¨åœ¨1992å¹´5æœˆèµ¢å¾—é’å¹´æ¯ã€‚\n",
      "[Model 2 (ZeroShot)]: åŒ…æ‹¬ç‘å®‰(ç•¥å¾®æ˜¯ä»–ä»¬çš„é«˜å¹´çº§)çš„ä¸€äº›çƒå‘˜åœ¨1992å¹´5æœˆå¸®åŠ©ä¿±ä¹éƒ¨èµ¢å¾—äº†é’æ˜¥æ¯,\n",
      "[Model 3 (Synthetic)]: åŒ…æ‹¬ç‘å®‰(ç•¥å¾®æ˜¯ä»–ä»¬çš„é«˜å¹´çº§)çš„çƒå‘˜åœ¨1992å¹´5æœˆå¸®åŠ©ä¿±ä¹éƒ¨èµ¢å¾—äº†é’æ˜¥æ¯,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01e12c1be8e4d05980415ac74f62298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  12%|#2        | 52.4M/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\" FINAL RESEARCH RESULTS \")\n",
    "print(\"=\" * 40)\n",
    "df_results = pd.DataFrame(results_table)\n",
    "display(df_results)\n",
    "\n",
    "# Save for your paper\n",
    "df_results.to_csv(\"report_data/final_metrics_table.csv\", index=False)\n",
    "\n",
    "# Optional: Show a few qualitative examples\n",
    "print(\"\\n--- Qualitative Analysis (First 3 Examples) ---\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nInput: {inputs[i]}\")\n",
    "    for model_name, preds in model_outputs.items():\n",
    "        print(f\"[{model_name}]: {preds[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
